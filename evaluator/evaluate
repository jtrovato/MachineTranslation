#!/usr/bin/env python
import sys
import argparse # optparse is deprecated
from itertools import islice, izip # slicing for iterators
from random import random
from nltk.corpus import wordnet, stopwords
from time import time
from collections import defaultdict

#create my english dictionary (global variable)
english_list =  open("/usr/share/dict/words").read().split()
english_dict = defaultdict(str)
for word in english_list:
    english_dict[word] = True
 
def word_matches(h, ref):
    matches = sum(1 for w in h if w in ref)
    misfits = [w for w in h if w not in ref]
    added = 0
    for word in misfits:
        try:
            added = sum(1 for ss in wordnet.synsets(word) if str(ss.lemma_names()[0]) in ref)
        except (UnicodeDecodeError, IOError):
            pass

    return max(len(h), matches+added)


def draw_num():
    n = random()
    if n < .4:
        return -1
    elif n < .8:
        return 1
    else:
        return 0

def meteor_score(h, ref):
    alpha = 0.7
    beta = 0.8
    gamma = 0.0
    matched_unigrams = word_matches(h,ref);
    intersection =  list(set(h) & set(ref));# this could be losing information for repeated unigrams
    precision = len(intersection)/float(len(h))
    recall = len(intersection)/float(len(ref))

    #check if all words are english
    for word in h:
        if word in english_dict:
            reduction = 0.9
        else:
            reduction = 1
    
    #chunking
    num_chunks = 0
    '''
    for i in xrange(len(h)):
        for j in xrange(len(ref)):
            if h[i] == ref[j]:
                k = 1
                while j+k < len(ref) and i+k < len(h):
                    if h[i:i+k] == ref[j:j+k]:
                        k += 1
                        
                    else:
                        i = i+k-1
                        break
            num_chunks += 1
    '''
    if len(intersection) == 0:
        #sys.stderr.write('len i ' + str(len(intersection)) + '\n')

        return 0 
    else:
        #sys.stderr.write('recall ' + str(recall) + '\n')
        #sys.stderr.write('precision ' + str(precision) + '\n')


        return reduction*(1-gamma*(num_chunks/float(matched_unigrams))**beta)*((precision*recall)/float(((1-alpha)*recall + alpha*precision)))
 
def main():
    parser = argparse.ArgumentParser(description='Evaluate translation hypotheses.')
    parser.add_argument('-i', '--input', default='data/hyp1-hyp2-ref',
            help='input file (default data/hyp1-hyp2-ref)')
    parser.add_argument('-n', '--num_sentences', default=None, type=int,
            help='Number of hypothesis pairs to evaluate')
    # note that if x == [1, 2, 3], then x[:None] == x[:] == x (copy); no need for sys.maxint
    opts = parser.parse_args()
 
    # we create a generator and avoid loading all sentences into a list
    def sentences():
        with open(opts.input) as f:
            for pair in f:
                yield [sentence.strip().split() for sentence in pair.split(' ||| ')]
 
    # note: the -n option does not work in the original code
    i = 0
    tot_sent = 51100
    start_time = time()



    for h1, h2, ref in islice(sentences(), opts.num_sentences):
        i += 1
        tr = ((time()-start_time)/i)*(tot_sent-i);
        sys.stderr.write("\r sentence %i  time remaining: %i" % (i,tr))
        rset = set(ref)
        #score1 = word_matches(h1, rset)
        #score2 = word_matches(h2, rset)
        score1 = meteor_score(h1, ref)
        score2 = meteor_score(h2, ref)


        if score1 > score2:
            print 1
        elif score1 < score2:
            print -1
        else:
            print -1
 
# convention to allow import of this file as a module
if __name__ == '__main__':
    main()
